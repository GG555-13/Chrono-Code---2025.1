{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "# --- 1. Definição do Ambiente (A Pista Virtual) ---\n",
        "# A pista é representada como uma sequência de estados. Cada estado é um trecho da pista.\n",
        "# Vamos mapear cada trecho (estado) para um índice numérico para facilitar o uso na Q-table.\n",
        "\n",
        "estados = {\n",
        "    \"Reta Inicial\": 0,\n",
        "    \"Curva Fechada\": 1,\n",
        "    \"Trilho Magnético\": 2,\n",
        "    \"Salto sobre Abismo\": 3,\n",
        "    \"Looping 360\": 4,\n",
        "    \"Linha de Chegada\": 5,\n",
        "    \"Abismo (Fim de Jogo)\": 6\n",
        "}\n",
        "\n",
        "# Para facilitar a leitura, criamos um mapa inverso\n",
        "nomes_estados = {v: k for k, v in estados.items()}\n",
        "\n",
        "# As ações que a IronFox pode tomar em qualquer estado.\n",
        "# Mapeamos cada ação para um índice numérico.\n",
        "acoes = {\n",
        "    \"Acelerar\": 0,\n",
        "    \"Manter Velocidade\": 1,\n",
        "    \"Frear\": 2\n",
        "}\n",
        "nomes_acoes = {v: k for k, v in acoes.items()}"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "U1Nbk8NwzR-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Tabela de Recompensas ---\n",
        "# Esta matriz define a recompensa (ou penalidade) por estar em um estado e ir para o próximo.\n",
        "# A estrutura é: R[estado_atual, proximo_estado]\n",
        "# Usamos -1 para indicar uma transição impossível (ex: ir da Reta Inicial direto para o Looping).\n",
        "# Recompensas:\n",
        "# - Negativas (penalidades): Ações que levam a um beco sem saída ou ao abismo.\n",
        "# - Positivas: Ações que progridem na pista.\n",
        "# - Recompensa máxima: Cruzar a linha de chegada.\n",
        "\n",
        "# Inicializamos com -1 para representar conexões inválidas\n",
        "R = np.full((len(estados), len(estados)), -1)\n",
        "\n",
        "# Definindo as transições válidas e suas recompensas\n",
        "R[estados[\"Reta Inicial\"], estados[\"Curva Fechada\"]] = 5\n",
        "R[estados[\"Curva Fechada\"], estados[\"Trilho Magnético\"]] = 10\n",
        "R[estados[\"Trilho Magnético\"], estados[\"Salto sobre Abismo\"]] = 20\n",
        "R[estados[\"Trilho Magnético\"], estados[\"Abismo (Fim de Jogo)\"]] = -100 # Erro: não saltar\n",
        "R[estados[\"Salto sobre Abismo\"], estados[\"Looping 360\"]] = 30\n",
        "R[estados[\"Salto sobre Abismo\"], estados[\"Abismo (Fim de Jogo)\"]] = -100 # Erro: saltar errado\n",
        "R[estados[\"Looping 360\"], estados[\"Linha de Chegada\"]] = 50\n",
        "R[estados[\"Linha de Chegada\"], estados[\"Linha de Chegada\"]] = 100 # Recompensa final\n",
        "\n",
        "# Ações específicas que levam a recompensas/penalidades\n",
        "# Vamos criar uma matriz de recompensas mais detalhada: R[estado, acao]\n",
        "# Isso é mais realista para Q-Learning, onde a recompensa depende do par (estado, ação).\n",
        "\n",
        "# Inicializa com uma pequena penalidade para incentivar rotas mais rápidas\n",
        "recompensas_por_acao = np.full((len(estados), len(acoes)), -0.5)\n",
        "\n",
        "# Definindo recompensas específicas\n",
        "recompensas_por_acao[estados[\"Reta Inicial\"], acoes[\"Acelerar\"]] = 5\n",
        "recompensas_por_acao[estados[\"Curva Fechada\"], acoes[\"Frear\"]] = 10 # Ação correta para a curva\n",
        "recompensas_por_acao[estados[\"Curva Fechada\"], acoes[\"Acelerar\"]] = -20 # Ação errada\n",
        "recompensas_por_acao[estados[\"Trilho Magnético\"], acoes[\"Acelerar\"]] = 20 # Precisa de velocidade para o salto\n",
        "recompensas_por_acao[estados[\"Salto sobre Abismo\"], acoes[\"Manter Velocidade\"]] = 30 # Ação correta durante o salto\n",
        "recompensas_por_acao[estados[\"Looping 360\"], acoes[\"Acelerar\"]] = 50\n",
        "recompensas_por_acao[estados[\"Looping 360\"], acoes[\"Frear\"]] = -50\n",
        "recompensas_por_acao[estados[\"Linha de Chegada\"], :] = 100 # Recompensa máxima\n",
        "\n",
        "# Definindo as transições de estado baseadas na ação\n",
        "# transicoes[estado, acao] = proximo_estado\n",
        "transicoes = {}\n",
        "transicoes[(estados[\"Reta Inicial\"], acoes[\"Acelerar\"])] = estados[\"Curva Fechada\"]\n",
        "transicoes[(estados[\"Curva Fechada\"], acoes[\"Frear\"])] = estados[\"Trilho Magnético\"]\n",
        "transicoes[(estados[\"Curva Fechada\"], acoes[\"Acelerar\"])] = estados[\"Abismo (Fim de Jogo)\"] # Bateu na curva\n",
        "transicoes[(estados[\"Trilho Magnético\"], acoes[\"Acelerar\"])] = estados[\"Salto sobre Abismo\"]\n",
        "transicoes[(estados[\"Trilho Magnético\"], acoes[\"Frear\"])] = estados[\"Abismo (Fim de Jogo)\"] # Não pegou velocidade\n",
        "transicoes[(estados[\"Salto sobre Abismo\"], acoes[\"Manter Velocidade\"])] = estados[\"Looping 360\"]\n",
        "transicoes[(estados[\"Salto sobre Abismo\"], acoes[\"Acelerar\"])] = estados[\"Abismo (Fim de Jogo)\"] # Perdeu controle\n",
        "transicoes[(estados[\"Salto sobre Abismo\"], acoes[\"Frear\"])] = estados[\"Abismo (Fim de Jogo)\"] # Caiu no abismo\n",
        "transicoes[(estados[\"Looping 360\"], acoes[\"Acelerar\"])] = estados[\"Linha de Chegada\"]\n"
      ],
      "metadata": {
        "id": "5Is1FkAC0Pgi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Implementação do Algoritmo Q-Learning ---\n",
        "\n",
        "# Inicialização da Q-table com zeros\n",
        "# Dimensões: (número de estados x número de ações)\n",
        "Q = np.zeros((len(estados), len(acoes)))\n",
        "\n",
        "# Hiperparâmetros do algoritmo\n",
        "alpha = 0.1      # Taxa de aprendizado (learning rate)\n",
        "gamma = 0.9      # Fator de desconto (valoriza recompensas futuras)\n",
        "epsilon = 1.0    # Taxa de exploração (exploration rate) - começa em 100%\n",
        "epsilon_decay = 0.001 # Fator de decaimento do epsilon\n",
        "num_episodios = 5000 # Número de simulações de corrida\n",
        "\n",
        "print(\"--- Iniciando o Treinamento da IronFox ---\")\n",
        "\n",
        "for i in range(num_episodios):\n",
        "    estado_atual = estados[\"Reta Inicial\"]\n",
        "    terminou = False\n",
        "\n",
        "    while not terminou:\n",
        "        # Escolha da ação: Exploração vs. Exploitation\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            acao_escolhida = random.choice(list(acoes.values())) # Explorar: escolhe ação aleatória\n",
        "        else:\n",
        "            acao_escolhida = np.argmax(Q[estado_atual, :]) # Exploitar: escolhe a melhor ação conhecida\n",
        "\n",
        "        # Simular a ação e obter o próximo estado e a recompensa\n",
        "        if (estado_atual, acao_escolhida) in transicoes:\n",
        "            proximo_estado = transicoes[(estado_atual, acao_escolhida)]\n",
        "            recompensa = recompensas_por_acao[estado_atual, acao_escolhida]\n",
        "        else:\n",
        "            # Ação inválida para o estado atual leva a um fim de jogo\n",
        "            proximo_estado = estados[\"Abismo (Fim de Jogo)\"]\n",
        "            recompensa = -100\n",
        "\n",
        "        # Atualização da Q-table usando a equação de Bellman\n",
        "        valor_q_antigo = Q[estado_atual, acao_escolhida]\n",
        "        proximo_max_q = np.max(Q[proximo_estado, :])\n",
        "\n",
        "        novo_valor_q = valor_q_antigo + alpha * (recompensa + gamma * proximo_max_q - valor_q_antigo)\n",
        "        Q[estado_atual, acao_escolhida] = novo_valor_q\n",
        "\n",
        "        estado_atual = proximo_estado\n",
        "\n",
        "        # Condição de término do episódio\n",
        "        if estado_atual == estados[\"Linha de Chegada\"] or estado_atual == estados[\"Abismo (Fim de Jogo)\"]:\n",
        "            terminou = True\n",
        "\n",
        "    # Decaimento do Epsilon: com o tempo, o agente explora menos e confia mais no que aprendeu\n",
        "    epsilon = max(0.01, epsilon * (1 - epsilon_decay))\n",
        "\n",
        "    if (i + 1) % 500 == 0:\n",
        "        print(f\"Episódio {i+1}/{num_episodios} concluído. Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "print(\"\\n--- Treinamento Concluído! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRpKlZUg0QZv",
        "outputId": "c06436dc-20dc-4633-85b7-039c239158b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iniciando o Treinamento da IronFox ---\n",
            "Episódio 500/5000 concluído. Epsilon: 0.606\n",
            "Episódio 1000/5000 concluído. Epsilon: 0.368\n",
            "Episódio 1500/5000 concluído. Epsilon: 0.223\n",
            "Episódio 2000/5000 concluído. Epsilon: 0.135\n",
            "Episódio 2500/5000 concluído. Epsilon: 0.082\n",
            "Episódio 3000/5000 concluído. Epsilon: 0.050\n",
            "Episódio 3500/5000 concluído. Epsilon: 0.030\n",
            "Episódio 4000/5000 concluído. Epsilon: 0.018\n",
            "Episódio 4500/5000 concluído. Epsilon: 0.011\n",
            "Episódio 5000/5000 concluído. Epsilon: 0.010\n",
            "\n",
            "--- Treinamento Concluído! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Análise e Demonstração ---\n",
        "\n",
        "print(\"\\nQ-Table Final (Valores aprendidos pela IronFox):\")\n",
        "print(\"Cada valor representa a 'qualidade' de uma ação em um determinado estado.\")\n",
        "print(Q)\n",
        "\n",
        "\n",
        "print(\"\\n--- Simulação da Corrida Perfeita (Política Ótima) ---\")\n",
        "print(\"Usando o conhecimento da Q-Table para pilotar sem erros.\\n\")\n",
        "\n",
        "estado_atual = estados[\"Reta Inicial\"]\n",
        "rota_otima = [nomes_estados[estado_atual]]\n",
        "recompensa_total = 0\n",
        "\n",
        "while estado_atual != estados[\"Linha de Chegada\"] and estado_atual != estados[\"Abismo (Fim de Jogo)\"]:\n",
        "    # Escolher a melhor ação possível (sem exploração)\n",
        "    acao_otima = np.argmax(Q[estado_atual, :])\n",
        "\n",
        "    print(f\"Estado Atual: '{nomes_estados[estado_atual]}'\")\n",
        "    print(f\"Ação Escolhida pela IronFox: '{nomes_acoes[acao_otima]}'\")\n",
        "\n",
        "    # Obter próximo estado e recompensa\n",
        "    proximo_estado = transicoes.get((estado_atual, acao_otima), estados[\"Abismo (Fim de Jogo)\"])\n",
        "    recompensa = recompensas_por_acao[estado_atual, acao_otima]\n",
        "    recompensa_total += recompensa\n",
        "\n",
        "    print(f\"Recompensa da Ação: {recompensa}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    estado_atual = proximo_estado\n",
        "    rota_otima.append(nomes_estados[estado_atual])\n",
        "    time.sleep(1) # Pausa para visualização\n",
        "\n",
        "print(f\"\\nResultado Final: A IronFox chegou na '{nomes_estados[estado_atual]}'\")\n",
        "print(f\"Rota Percorrida: {' -> '.join(rota_otima)}\")\n",
        "print(f\"Recompensa Total da Corrida: {recompensa_total:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyagcnAB0ZN0",
        "outputId": "bdbaf90f-4d9a-4cf5-d0cb-be5a369dd9d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q-Table Final (Valores aprendidos pela IronFox):\n",
            "Cada valor representa a 'qualidade' de uma ação em um determinado estado.\n",
            "[[  84.875      -100.         -100.        ]\n",
            " [ -20.          -99.99999994   88.75      ]\n",
            " [  87.5         -99.99999912   -0.49999998]\n",
            " [  -0.49999949   75.           -0.49999855]\n",
            " [  50.          -99.98407321  -99.99806367]\n",
            " [   0.            0.            0.        ]\n",
            " [   0.            0.            0.        ]]\n",
            "\n",
            "--- Simulação da Corrida Perfeita (Política Ótima) ---\n",
            "Usando o conhecimento da Q-Table para pilotar sem erros.\n",
            "\n",
            "Estado Atual: 'Reta Inicial'\n",
            "Ação Escolhida pela IronFox: 'Acelerar'\n",
            "Recompensa da Ação: 5.0\n",
            "--------------------\n",
            "Estado Atual: 'Curva Fechada'\n",
            "Ação Escolhida pela IronFox: 'Frear'\n",
            "Recompensa da Ação: 10.0\n",
            "--------------------\n",
            "Estado Atual: 'Trilho Magnético'\n",
            "Ação Escolhida pela IronFox: 'Acelerar'\n",
            "Recompensa da Ação: 20.0\n",
            "--------------------\n",
            "Estado Atual: 'Salto sobre Abismo'\n",
            "Ação Escolhida pela IronFox: 'Manter Velocidade'\n",
            "Recompensa da Ação: 30.0\n",
            "--------------------\n",
            "Estado Atual: 'Looping 360'\n",
            "Ação Escolhida pela IronFox: 'Acelerar'\n",
            "Recompensa da Ação: 50.0\n",
            "--------------------\n",
            "\n",
            "Resultado Final: A IronFox chegou na 'Linha de Chegada'\n",
            "Rota Percorrida: Reta Inicial -> Curva Fechada -> Trilho Magnético -> Salto sobre Abismo -> Looping 360 -> Linha de Chegada\n",
            "Recompensa Total da Corrida: 115.00\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}